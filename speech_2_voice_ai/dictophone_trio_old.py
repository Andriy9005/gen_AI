import os
import sounddevice as sd
from scipy.io.wavfile import write
import whisper
import pandas as pd
from jiwer import wer
import language_tool_python
import time
import torch
from transformers import MarianMTModel, MarianTokenizer
import time

# üìç –î–æ–¥–∞—Ç–∏ ffmpeg –¥–æ —à–ª—è—Ö—É (—è–∫—â–æ –ª–æ–∫–∞–ª—å–Ω–æ)
ffmpeg_path = r"C:\\Users\\Andriy.Bespalyy\\ffmpeg\\bin"
os.environ["PATH"] = ffmpeg_path + os.pathsep + os.environ["PATH"]

# üéôÔ∏è –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∞—É–¥—ñ–æ
duration = 10
sample_rate = 44100
device_index = 8
output_file = "mic_recording.wav"

print(f"üé§ Recording will start in 2 seconds...")
time.sleep(2)  # ‚è≥ –ü–∞—É–∑–∞ 2 —Å–µ–∫—É–Ω–¥–∏

print(f"üé§ Recording for {duration} seconds from device #{device_index}...")
recording = sd.rec(
    int(duration * sample_rate),
    samplerate=sample_rate,
    channels=1,
    dtype='int16',
    device=device_index
)
sd.wait()
write(output_file, sample_rate, recording)
print(f"‚úÖ Audio saved as {output_file}")

# üéß –í—ñ–¥—Ç–≤–æ—Ä–µ–Ω–Ω—è
sd.play(recording, samplerate=sample_rate)
sd.wait()

# üß† –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ Whisper
model = whisper.load_model("medium")

# üß† –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
print("üß† Detecting language...")

lang_result = model.transcribe(output_file, task="transcribe", language=None)

# ‚ú® –í–∏—Ç—è–≥—É—î–º–æ –≤–∏–∑–Ω–∞—á–µ–Ω—É Whisper –º–æ–≤—É
raw_lang = lang_result.get("language", "unknown")

# ‚úÖ –Ø–∫—â–æ –Ω–µ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∞ ‚Äî –≤–≤–∞–∂–∞—î–º–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é
detected_lang = "en" if raw_lang == "en" else "uk"

print(f"üåê Detected language (forced logic): {detected_lang}")

# –ü—ñ–¥–∫–ª—é—á–∞—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π LanguageTool
if detected_lang == "uk":
    tool = language_tool_python.LanguageToolPublicAPI('uk-UA')
elif detected_lang == "en":
    tool = language_tool_python.LanguageToolPublicAPI('en-US')
else:
    print(f"‚ö†Ô∏è Unsupported language: {detected_lang}. Skipping grammar correction.")
    tool = None

# ‚úçÔ∏è –§—ñ–Ω–∞–ª—å–Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –∑ –≤—ñ–¥–æ–º–æ—é –º–æ–≤–æ—é
print("üìù Transcribing...")
result = model.transcribe(output_file, language=detected_lang)
raw_text = result["text"]

# üõ† –ì—Ä–∞–º–∞—Ç–∏—á–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è
def correct_text(text):
    if tool is None:
        return text
    matches = tool.check(text)
    return language_tool_python.utils.correct(text, matches)

corrected = correct_text(raw_text)
print(f"‚úÖ Corrected transcript:\n{corrected}")

# üåê –ü–µ—Ä–µ–∫–ª–∞–¥ –º–æ–¥–µ–ª–ª—é MarianMT
if detected_lang == "uk":
    trans_model_path = "C:/Users/Andriy.Bespalyy/Desktop/models/marianmt-uk-en-hplt-final/marianmt-uk-en-hplt-final"
elif detected_lang == "en":
    trans_model_path = "C:/Users/Andriy.Bespalyy/Desktop/models/marianmt-en-uk-hplt-final/marianmt-en-uk-hplt-final"
else:
    trans_model_path = None

if trans_model_path:
    tokenizer = MarianTokenizer.from_pretrained(trans_model_path)
    translator = MarianMTModel.from_pretrained(trans_model_path)
    inputs = tokenizer([corrected], return_tensors="pt", padding=True, truncation=True).to(translator.device)
    with torch.no_grad():
        translated = translator.generate(**inputs)
    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)
    print(f"üåç Translated text:\n{translated_text}")
else:
    translated_text = "(no translation)"

# üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ CSV
transcripts = [{"file": output_file, "transcript": corrected, "translation": translated_text}]
df = pd.DataFrame(transcripts)
df.to_csv("transcriptions_corrected_translated.csv", index=False, encoding="utf-8-sig")
print("üìÑ Saved to transcriptions_corrected_translated.csv")

# üìè WER (—è–∫—â–æ ground-truth –¥–æ—Å—Ç—É–ø–Ω–∏–π)
ground_truths = {
    "mic_recording.wav": "–û—á—ñ–∫—É–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç —Ç—É—Ç –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç—ñ"
}

if output_file in ground_truths:
    gt = ground_truths[output_file]
    print(f"üîç Ground truth: {gt}")
    print(f"üìù Predicted   : {corrected}")
    error = wer(gt, corrected)
    df_wer = pd.DataFrame([{"file": output_file, "WER": error}])
    df_wer.to_csv("wer_scores.csv", index=False, encoding="utf-8-sig")
    print(f"üìâ WER saved: {error:.2%}")
else:
    print("‚ÑπÔ∏è Ground-truth not provided ‚Üí WER not calculated.")




# üìÅ –®–ª—è—Ö –¥–æ CSV-—Ñ–∞–π–ª—É
csv_file = "transcriptions_corrected_translated.csv"
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

# üß† –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏ –ø—Ä–æ—Å—Ç–æ –∑ —Ç–µ–∫—Å—Ç—É
from langdetect import detect

def detect_lang(text):
    try:
        return detect(text)
    except:
        return "unknown"



import os
import csv
import torch
import sounddevice as sd
from scipy.io.wavfile import write

from nemo.collections.tts.models import FastPitchModel, HifiGanModel

# üéØ –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∏—Å—Ç—Ä–æ—é
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π...")

# –ê–Ω–≥–ª—ñ–π—Å—å–∫—ñ –º–æ–¥–µ–ª—ñ (–º–æ–∂–Ω–∞ –∑–∞–ª–∏—à–∏—Ç–∏, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ)
fastpitch_en = FastPitchModel.from_pretrained("tts_en_fastpitch").to(device)
hifigan_en = HifiGanModel.from_pretrained("tts_en_hifigan").to(device)

# –£–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –º–æ–¥–µ–ª—ñ –∑ Hugging Face
fastpitch_uk = FastPitchModel.from_pretrained("theodotus/tts_uk_fastpitch").to(device)
hifigan_uk = HifiGanModel.from_pretrained("theodotus/tts_uk_hifigan").to(device)

# üîä –°–∏–Ω—Ç–µ–∑ + –ø—Ä–æ–≥—Ä–∞–≤–∞–Ω–Ω—è
def synthesize_and_play(text, path, lang_code="en"):
    if lang_code == "en":
        fastpitch, hifigan = fastpitch_en, hifigan_en
    else:
        fastpitch, hifigan = fastpitch_uk, hifigan_uk

    sample_rate = 22050  # ‚úÖ –ß–∞—Å—Ç–æ—Ç–∞ HiFi-GAN –º–æ–¥–µ–ª–µ–π
    with torch.no_grad():
        parsed = fastpitch.parse(text)
        spectrogram = fastpitch.generate_spectrogram(tokens=parsed)
        audio = hifigan.convert_spectrogram_to_audio(spec=spectrogram)
        audio_np = audio[0].cpu().numpy()

    write(path, sample_rate, audio_np)
    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ —É {path}")
    sd.play(audio_np, samplerate=sample_rate)
    sd.wait()

# üìÑ –û–±—Ä–æ–±–∫–∞ CSV
csv_file = "transcriptions_corrected_translated.csv"
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

with open(csv_file, newline='', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    for i, row in enumerate(reader):
        text = row.get("translation", "").strip()
        if not text:
            continue
        try:
            lang_code = detect(text)
        except:
            lang_code = "en"
        lang_code = "uk" if lang_code == "uk" else "en"
        filename = os.path.join(output_dir, f"translated_{i+1}.wav")
        print(f"üîπ –û–∑–≤—É—á–µ–Ω–Ω—è ({lang_code.upper()}): {text}")
        synthesize_and_play(text, filename, lang_code=lang_code)
        break  # üîÅ –¢—ñ–ª—å–∫–∏ –ø–µ—Ä—à–∏–π —Ä—è–¥–æ–∫
 