{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a91a72-9d87-4866-90d8-2fc5a2da010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Fine-tuning MarianMT on Dell 7567 (local GPU: GTX)\n",
    "!pip uninstall keras\n",
    "!pip install tf-keras\n",
    "\n",
    "from transformers import MarianTokenizer, MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "from evaluate import load as load_metric\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ‚úÖ –®–ª—è—Ö –¥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª—É\n",
    "file_path = \"C://Users//skyjet//Downloads//en-uk_dataset.tsv\"  # –ó–∞–º—ñ–Ω–∏—Ç–∏ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω–∏–π –ª–æ–∫–∞–ª—å–Ω–∏–π —à–ª—è—Ö\n",
    "\n",
    "# üìä –û–±—Ä–æ–±–∫–∞ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É\n",
    "sample_size = 100_000\n",
    "chunk_size = 50_000\n",
    "samples = []\n",
    "total_collected = 0\n",
    "\n",
    "for chunk in pd.read_csv(file_path, sep=\"\\t\", names=[\"en\", \"uk\"], quoting=3, chunksize=chunk_size):\n",
    "    chunk = chunk.dropna()\n",
    "    chunk = chunk[\n",
    "        chunk[\"en\"].str.len().between(4, 256) &\n",
    "        chunk[\"uk\"].str.len().between(4, 256)\n",
    "    ]\n",
    "    available = len(chunk)\n",
    "    need = sample_size - total_collected\n",
    "    if available > need:\n",
    "        chunk = chunk.sample(n=need, random_state=42)\n",
    "    samples.append(chunk)\n",
    "    total_collected += len(chunk)\n",
    "    if total_collected >= sample_size:\n",
    "        break\n",
    "\n",
    "df = pd.concat(samples).reset_index(drop=True)\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.05)\n",
    "\n",
    "# üî† –ú–æ–¥–µ–ª—å + —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-uk\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# üßº –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è\n",
    "def preprocess(example):\n",
    "    model_inputs = tokenizer(example[\"en\"], max_length=256, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(example[\"uk\"], max_length=256, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# üìè BLEU\n",
    "bleu = load_metric(\"sacrebleu\")\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "\n",
    "# ‚öôÔ∏è –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (GTX-compatible)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./marianmt-en-uk-hplt\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=6,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    label_smoothing_factor=0.1,\n",
    "    fp16=False,  # GTX 10xx does not fully support native AMP\n",
    ")\n",
    "\n",
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./marianmt-en-uk-hplt-final\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
